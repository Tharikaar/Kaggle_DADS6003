{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2404def3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name : Tharika Arjitnupap \n",
    "# ID : 6410422011 \n",
    "# rank 5 <<< tharikaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0a441b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948bf705",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../input/dads6003-in-class-competition/train.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c48302f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('../input/dads6003-in-class-competition/test.csv')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353b3d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4361b1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0b79b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.dropna()\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f484c31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "corrmat = df_train.corr()\n",
    "top_corr_features = corrmat.index\n",
    "plt.figure(figsize=(20,20))\n",
    "#plot heat map\n",
    "sns.heatmap(df_train[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c511e182",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(\"y\", axis=1)\n",
    "y_train = df_train[\"y\"]\n",
    "X_test  = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc774469",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression()\n",
    "\n",
    "scoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\n",
    "scores = cross_validate(LR, X_train, y_train, scoring=scoring, cv=20)\n",
    "\n",
    "sorted(scores.keys())\n",
    "LR_fit_time = scores['fit_time'].mean()\n",
    "LR_score_time = scores['score_time'].mean()\n",
    "LR_accuracy = scores['test_accuracy'].mean()\n",
    "LR_precision = scores['test_precision_macro'].mean()\n",
    "LR_recall = scores['test_recall_macro'].mean()\n",
    "LR_f1 = scores['test_f1_weighted'].mean()\n",
    "LR_roc = scores['test_roc_auc'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b8fa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = DecisionTreeClassifier()\n",
    "\n",
    "scoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\n",
    "scores = cross_validate(decision_tree, X_train, y_train, scoring=scoring, cv=20)\n",
    "\n",
    "sorted(scores.keys())\n",
    "dtree_fit_time = scores['fit_time'].mean()\n",
    "dtree_score_time = scores['score_time'].mean()\n",
    "dtree_accuracy = scores['test_accuracy'].mean()\n",
    "dtree_precision = scores['test_precision_macro'].mean()\n",
    "dtree_recall = scores['test_recall_macro'].mean()\n",
    "dtree_f1 = scores['test_f1_weighted'].mean()\n",
    "dtree_roc = scores['test_roc_auc'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501c8b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = SVC(probability = True)\n",
    "\n",
    "scoring = ['accuracy','precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\n",
    "scores = cross_validate(SVM, X_train, y_train, scoring=scoring, cv=20)\n",
    "\n",
    "sorted(scores.keys())\n",
    "SVM_fit_time = scores['fit_time'].mean()\n",
    "SVM_score_time = scores['score_time'].mean()\n",
    "SVM_accuracy = scores['test_accuracy'].mean()\n",
    "SVM_precision = scores['test_precision_macro'].mean()\n",
    "SVM_recall = scores['test_recall_macro'].mean()\n",
    "SVM_f1 = scores['test_f1_weighted'].mean()\n",
    "SVM_roc = scores['test_roc_auc'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73148529",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN = KNeighborsClassifier()\n",
    "\n",
    "scoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\n",
    "scores = cross_validate(KNN, X_train, y_train, scoring=scoring, cv=20)\n",
    "\n",
    "sorted(scores.keys())\n",
    "KNN_fit_time = scores['fit_time'].mean()\n",
    "KNN_score_time = scores['score_time'].mean()\n",
    "KNN_accuracy = scores['test_accuracy'].mean()\n",
    "KNN_precision = scores['test_precision_macro'].mean()\n",
    "KNN_recall = scores['test_recall_macro'].mean()\n",
    "KNN_f1 = scores['test_f1_weighted'].mean()\n",
    "KNN_roc = scores['test_roc_auc'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f81685",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "scoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\n",
    "scores = cross_validate(random_forest, X_train, y_train, scoring=scoring, cv=20)\n",
    "\n",
    "sorted(scores.keys())\n",
    "forest_fit_time = scores['fit_time'].mean()\n",
    "forest_score_time = scores['score_time'].mean()\n",
    "forest_accuracy = scores['test_accuracy'].mean()\n",
    "forest_precision = scores['test_precision_macro'].mean()\n",
    "forest_recall = scores['test_recall_macro'].mean()\n",
    "forest_f1 = scores['test_f1_weighted'].mean()\n",
    "forest_roc = scores['test_roc_auc'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58f6983",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA = LinearDiscriminantAnalysis()\n",
    "\n",
    "scoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\n",
    "scores = cross_validate(LDA, X_train, y_train, scoring=scoring, cv=20)\n",
    "\n",
    "sorted(scores.keys())\n",
    "LDA_fit_time = scores['fit_time'].mean()\n",
    "LDA_score_time = scores['score_time'].mean()\n",
    "LDA_accuracy = scores['test_accuracy'].mean()\n",
    "LDA_precision = scores['test_precision_macro'].mean()\n",
    "LDA_recall = scores['test_recall_macro'].mean()\n",
    "LDA_f1 = scores['test_f1_weighted'].mean()\n",
    "LDA_roc = scores['test_roc_auc'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1564385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "QDA = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "scoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\n",
    "scores = cross_validate(QDA, X_train, y_train, scoring=scoring, cv=20)\n",
    "\n",
    "sorted(scores.keys())\n",
    "QDA_fit_time = scores['fit_time'].mean()\n",
    "QDA_score_time = scores['score_time'].mean()\n",
    "QDA_accuracy = scores['test_accuracy'].mean()\n",
    "QDA_precision = scores['test_precision_macro'].mean()\n",
    "QDA_recall = scores['test_recall_macro'].mean()\n",
    "QDA_f1 = scores['test_f1_weighted'].mean()\n",
    "QDA_roc = scores['test_roc_auc'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c591869d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_initial = pd.DataFrame({\n",
    "    'Model'       : ['Logistic Regression', 'Decision Tree', 'Support Vector Machine', 'K-Nearest Neighbors', 'Random Forest', 'Linear Discriminant Analysis', 'Quadratic Discriminant Analysis'],\n",
    "    'Fitting time': [LR_fit_time, dtree_fit_time, SVM_fit_time, KNN_fit_time, forest_fit_time, LDA_fit_time, QDA_fit_time],\n",
    "    'Scoring time': [LR_score_time, dtree_score_time, SVM_score_time, KNN_score_time, forest_score_time, LDA_score_time, QDA_score_time],\n",
    "    'Accuracy'    : [LR_accuracy, dtree_accuracy, SVM_accuracy, KNN_accuracy, forest_accuracy, LDA_accuracy, QDA_accuracy],\n",
    "    'Precision'   : [LR_precision, dtree_precision, SVM_precision, KNN_precision, forest_precision, LDA_precision, QDA_precision],\n",
    "    'Recall'      : [LR_recall, dtree_recall, SVM_recall, KNN_recall, forest_recall, LDA_recall, QDA_recall],\n",
    "    'F1_score'    : [LR_f1, dtree_f1, SVM_f1, KNN_f1, forest_f1, LDA_f1, QDA_f1],\n",
    "    'AUC_ROC'     : [LR_roc, dtree_roc, SVM_roc, KNN_roc, forest_roc, LDA_roc, QDA_roc],\n",
    "    }, columns = ['Model', 'Fitting time', 'Scoring time', 'Accuracy', 'Precision', 'Recall', 'F1_score', 'AUC_ROC'])\n",
    "\n",
    "models_initial.sort_values(by='Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3d0f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output\n",
    "QDA = QDA.fit(X_train, y_train)\n",
    "predict = QDA.predict(X_test)\n",
    "fields = ['id','Expected']\n",
    "df = pd.DataFrame()\n",
    "id = list(range(1,2501))\n",
    "df['id']=pd.Series(id)\n",
    "df['Expected']=pd.Series(predict)\n",
    "df.to_csv('submit_QDA.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
